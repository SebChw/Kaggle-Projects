{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I started this competition only few days before the competition ended it is not perfect. \n",
    "\n",
    "When I saw 250 attributes I immadiately got an idea to use Triplet Network for dimensionality reduction and then use XGBoost classifier on that lower dimensional space of attributes as I've never used it before and I've heard that it does really good job. There are 1000000 entries so one probably can use very powerfull models and not be afraid of overfitting. However, I didn't have enough time to do it so I randomly selected only small part of entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = './data'\n",
    "file_name = os.listdir(folder)[0]\n",
    "\n",
    "def fetch_data(folder_name=folder, file_name=file_name):\n",
    "    zip_path = os.path.join(folder_name, file_name)\n",
    "    data_zip = ZipFile(zip_path)\n",
    "    data_zip.extractall(path=folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(os.path.join(folder, 'train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(os.path.join(folder, 'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>...</th>\n",
       "      <th>f276</th>\n",
       "      <th>f277</th>\n",
       "      <th>f278</th>\n",
       "      <th>f279</th>\n",
       "      <th>f280</th>\n",
       "      <th>f281</th>\n",
       "      <th>f282</th>\n",
       "      <th>f283</th>\n",
       "      <th>f284</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.205979</td>\n",
       "      <td>0.410993</td>\n",
       "      <td>0.176775</td>\n",
       "      <td>0.223581</td>\n",
       "      <td>0.423543</td>\n",
       "      <td>0.476140</td>\n",
       "      <td>0.413590</td>\n",
       "      <td>0.612021</td>\n",
       "      <td>0.534873</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.181004</td>\n",
       "      <td>0.473119</td>\n",
       "      <td>0.011734</td>\n",
       "      <td>0.213657</td>\n",
       "      <td>0.619678</td>\n",
       "      <td>0.441593</td>\n",
       "      <td>0.230407</td>\n",
       "      <td>0.686013</td>\n",
       "      <td>0.281971</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.182583</td>\n",
       "      <td>0.307431</td>\n",
       "      <td>0.325950</td>\n",
       "      <td>0.207116</td>\n",
       "      <td>0.605699</td>\n",
       "      <td>0.309695</td>\n",
       "      <td>0.493337</td>\n",
       "      <td>0.751107</td>\n",
       "      <td>0.536272</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.180240</td>\n",
       "      <td>0.494592</td>\n",
       "      <td>0.008367</td>\n",
       "      <td>0.223580</td>\n",
       "      <td>0.760618</td>\n",
       "      <td>0.439211</td>\n",
       "      <td>0.432055</td>\n",
       "      <td>0.776147</td>\n",
       "      <td>0.483958</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.177172</td>\n",
       "      <td>0.495513</td>\n",
       "      <td>0.014263</td>\n",
       "      <td>0.548819</td>\n",
       "      <td>0.625396</td>\n",
       "      <td>0.562493</td>\n",
       "      <td>0.117158</td>\n",
       "      <td>0.561255</td>\n",
       "      <td>0.077115</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 287 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id        f0        f1        f2        f3        f4        f5        f6  \\\n",
       "0   0  0.205979  0.410993  0.176775  0.223581  0.423543  0.476140  0.413590   \n",
       "1   1  0.181004  0.473119  0.011734  0.213657  0.619678  0.441593  0.230407   \n",
       "2   2  0.182583  0.307431  0.325950  0.207116  0.605699  0.309695  0.493337   \n",
       "3   3  0.180240  0.494592  0.008367  0.223580  0.760618  0.439211  0.432055   \n",
       "4   4  0.177172  0.495513  0.014263  0.548819  0.625396  0.562493  0.117158   \n",
       "\n",
       "         f7        f8  ...  f276  f277  f278  f279  f280  f281  f282  f283  \\\n",
       "0  0.612021  0.534873  ...     0     1     0     0     0     0     0     0   \n",
       "1  0.686013  0.281971  ...     0     1     0     0     0     0     0     0   \n",
       "2  0.751107  0.536272  ...     0     0     0     1     1     0     0     0   \n",
       "3  0.776147  0.483958  ...     0     0     0     0     1     0     0     0   \n",
       "4  0.561255  0.077115  ...     0     1     1     0     1     0     0     1   \n",
       "\n",
       "   f284  target  \n",
       "0     0       1  \n",
       "1     0       1  \n",
       "2     0       1  \n",
       "3     0       1  \n",
       "4     0       1  \n",
       "\n",
       "[5 rows x 287 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head() #A lot of attributes It seems like they are all normalized, at the end some one hot encoded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Columns: 287 entries, id to target\n",
      "dtypes: float64(240), int64(47)\n",
      "memory usage: 2.1 GB\n"
     ]
    }
   ],
   "source": [
    "training_data.info() #Some information won't be displayed by default if we have a lot of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.isna().sum(axis=0).sum() # No null values at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In case that we have very long dataset with a lot of attributes we can't visually inspect it.\n",
    "description = training_data.describe() \n",
    "description.drop(\"id\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max'], dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description.index # To know what we can search in the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "1    500485\n",
      "0    499515\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# So all attributes seems to be in the interval of [0,1]\n",
    "# Generally all attributes seems to be in a good condition, nothing to do with them\n",
    "print(description.loc['min'][description.loc['min'] < 0].sum())\n",
    "print(description.loc['max'][description.loc['max'] > 1].sum())\n",
    "#One sanity check may be to check distribution of the target attribute\n",
    "print(training_data['target'].value_counts()) # data is uniformly distributed so guessing randomly we should obtain 50% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>...</th>\n",
       "      <th>f276</th>\n",
       "      <th>f277</th>\n",
       "      <th>f278</th>\n",
       "      <th>f279</th>\n",
       "      <th>f280</th>\n",
       "      <th>f281</th>\n",
       "      <th>f282</th>\n",
       "      <th>f283</th>\n",
       "      <th>f284</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.214334</td>\n",
       "      <td>0.460218</td>\n",
       "      <td>0.129253</td>\n",
       "      <td>0.277598</td>\n",
       "      <td>0.580710</td>\n",
       "      <td>0.416619</td>\n",
       "      <td>0.386532</td>\n",
       "      <td>0.654858</td>\n",
       "      <td>0.462256</td>\n",
       "      <td>0.258031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250096</td>\n",
       "      <td>0.137164</td>\n",
       "      <td>0.144793</td>\n",
       "      <td>0.130667</td>\n",
       "      <td>0.139210</td>\n",
       "      <td>0.199331</td>\n",
       "      <td>0.156065</td>\n",
       "      <td>0.183741</td>\n",
       "      <td>0.15468</td>\n",
       "      <td>0.500485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.053320</td>\n",
       "      <td>0.101316</td>\n",
       "      <td>0.120805</td>\n",
       "      <td>0.063163</td>\n",
       "      <td>0.115338</td>\n",
       "      <td>0.058231</td>\n",
       "      <td>0.133457</td>\n",
       "      <td>0.065158</td>\n",
       "      <td>0.129439</td>\n",
       "      <td>0.119081</td>\n",
       "      <td>...</td>\n",
       "      <td>0.433068</td>\n",
       "      <td>0.344021</td>\n",
       "      <td>0.351892</td>\n",
       "      <td>0.337036</td>\n",
       "      <td>0.346166</td>\n",
       "      <td>0.399498</td>\n",
       "      <td>0.362917</td>\n",
       "      <td>0.387273</td>\n",
       "      <td>0.36160</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 286 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            f0        f1        f2        f3        f4        f5        f6  \\\n",
       "mean  0.214334  0.460218  0.129253  0.277598  0.580710  0.416619  0.386532   \n",
       "std   0.053320  0.101316  0.120805  0.063163  0.115338  0.058231  0.133457   \n",
       "\n",
       "            f7        f8        f9  ...      f276      f277      f278  \\\n",
       "mean  0.654858  0.462256  0.258031  ...  0.250096  0.137164  0.144793   \n",
       "std   0.065158  0.129439  0.119081  ...  0.433068  0.344021  0.351892   \n",
       "\n",
       "          f279      f280      f281      f282      f283     f284    target  \n",
       "mean  0.130667  0.139210  0.199331  0.156065  0.183741  0.15468  0.500485  \n",
       "std   0.337036  0.346166  0.399498  0.362917  0.387273  0.36160  0.500000  \n",
       "\n",
       "[2 rows x 286 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#But are they standardized? \n",
    "description.loc[['mean', 'std']] # No so we can try to standardize the attributes. However Let's firt split the data on two datasets. So that\n",
    "#We will standardize Validation set with the training mean not to be biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividing trainig_data on validation. One way to just select 20% of training data for validation. We can do it randomly as distributions of classes are almost identical\n",
    "#Preferably Ill use K-Fold-CV if I had more time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "training_set, validation_set = train_test_split(training_data, test_size=0.2, random_state=42)\n",
    "\n",
    "training_set_label = training_set['target']\n",
    "validation_set_label = validation_set['target']\n",
    "training_set.drop([\"id\", \"target\"], axis=1, inplace=True)\n",
    "validation_set.drop([\"id\", \"target\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    0.500622\n",
      "0    0.499377\n",
      "Name: target, dtype: float64\n",
      "0    0.500065\n",
      "1    0.499935\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# As we have a lot of data random sapmling gives almost the same distribution. But if we want to make sure that it will be the same\n",
    "#we can use stratified sampling\n",
    "print(training_set_label.value_counts() / len(training_set_label))\n",
    "print(validation_set_label.value_counts() / len(validation_set_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Could use ScikitLearn API but why not use pandas \n",
    "training_mean = training_set.mean(axis=0) #Axis 0 means that 0th axis (rows) will be collapsed. \n",
    "training_std = training_set.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = training_set.sub(training_mean, axis='columns').divide(training_std, axis='columns')\n",
    "validation_set = validation_set.sub(training_mean, axis='columns').divide(training_std, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-4.455470690245741e-17, -0.00014155983020388869, 1.0, 0.9999599812115656)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.mean(axis=0).mean(), validation_set.mean(axis=0).mean(), training_set.std(axis=0).mean(), validation_set.std(axis=0).mean()\n",
    "\n",
    "#Since these sets differ we are not able to ideally standardize validation set using training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will use triplet Networks to at first get only important parameters and try to separate probes as much as possible to make\n",
    "#it easier for classifier\n",
    "class ChemistryDataset(Dataset):\n",
    "    def __init__(self, predictors, labels):\n",
    "        self.indices = predictors.reset_index().index.values #Returns row labels for df\n",
    "        self.predictors = predictors.values #we need numpt array\n",
    "        self.labels = labels.values #We need numpy arrays at that moment\n",
    "    def __len__(self):\n",
    "        return len(self.predictors)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        anchor_label = self.labels[index] # To know which class assign to positive and negative in the triplet\n",
    "        anchor = torch.tensor(self.predictors[index], dtype=torch.float32)\n",
    "        \n",
    "        #Now we need candidates for positive, But we must Exclude currently considered index!\n",
    "        positive_candidates = self.indices[self.indices != index][self.labels[self.indices != index] == anchor_label]\n",
    "        positive_idx = random.choice(positive_candidates)\n",
    "        positive = torch.tensor(self.predictors[positive_idx], dtype=torch.float32)\n",
    "        \n",
    "        negative_candidates = self.indices[self.labels != anchor_label]\n",
    "        negative_idx = random.choice(negative_candidates)\n",
    "        negative = torch.tensor(self.predictors[negative_idx], dtype=torch.float32)\n",
    "        \n",
    "        return anchor, positive, negative, torch.tensor(anchor_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) torch.Size([285])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(1), torch.Size([285]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_ = ChemistryDataset(training_set[:10], training_set_label[:10])\n",
    "print(set_.__getitem__(8)[3], set_.__getitem__(8)[0].shape) #seems fine\n",
    "set_.__getitem__(0)[3], set_.__getitem__(0)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin = 1.0, type=\"L2\"):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.type= type\n",
    "        #our objective is distance(anchor, negative) - distance(anchor, positive) > margin -> we want to separate these probes\n",
    "        # moving margin to the right and multiplying by -1 we get: d(a,p) - d(a,n) + m < 0 # If this is not true then we generate Loss\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        if self.type==\"L2\":\n",
    "            pdist = nn.PairwiseDistance(p=2)\n",
    "            #there is also cdist function\n",
    "            distance_positive = pdist(anchor, positive) # by default it is Euclidean distance\n",
    "            distance_negative = pdist(anchor, negative)\n",
    "            losses = torch.relu(distance_positive - distance_negative + self.margin)\n",
    "        elif self.type == \"cosine\":\n",
    "            #In situations where there a many dimensions sometimes that loss may work better\n",
    "            cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "            similarity_positive = cos(anchor, positive)\n",
    "            similarity_negative = cos(anchor, negative)\n",
    "\n",
    "            losses = torch.relu(similarity_negative - similarity_positive + self.margin) #the goal it to achieve\n",
    "            #similarity(anchor,positive) - margin > similarity(anchor,negative)\n",
    "               \n",
    "        elif self.type == \"L1\":\n",
    "            #this loss is taken from some YT video where guy said that IBM invented it but I couldn't find paper\n",
    "            #https://www.youtube.com/watch?v=dG8le1YWUI8&t=1431s\n",
    "            distance_positive = torch.unsqueeze(torch.sum(torch.abs(anchor - positive), axis=1), 0)\n",
    "            distance_negative = torch.unsqueeze(torch.sum(torch.abs(anchor - negative), axis=1), 0)\n",
    "            concat = torch.cat((distance_positive, distance_negative), axis=0)\n",
    "            probs = torch.softmax(concat, axis=0)\n",
    "            loss = torch.mean(torch.abs(probs[0] + torch.abs(1.0 - probs[1])))\n",
    "\n",
    "            return loss\n",
    "            \n",
    "        return losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5500)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#! Sanity Check\n",
    "loss = TripletLoss(margin = 0.1)\n",
    "a = torch.tensor([[1, 0], [0 ,1]], dtype=torch.float)\n",
    "b = torch.tensor([[0, 0], [0 ,0]], dtype=torch.float)\n",
    "c = torch.tensor([[1, 0], [-1 ,0]], dtype=torch.float)\n",
    "loss.forward(a,b,c)  # First triplet generater loss 1.1 and second one 0 o it should be 0.55."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I tried some very simple Networks and then tried to make it bigger however the smaller one does better job\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Network, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(self.input_size, self.input_size*2, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(self.input_size*2), # A lot of people claim that BN should be after ReLu. Bot originally it was before\n",
    "            nn.Linear(self.input_size*2, self.input_size*4),\n",
    "            nn.ReLU(), \n",
    "            #nn.BatchNorm1d(self.input_size*4), # A lot of people claim that BN should be after ReLu. Bot originally it was before\n",
    "            nn.Linear(self.input_size*4, output_size),\n",
    "            # nn.ReLU(),\n",
    "            # nn.BatchNorm1d(self.input_size*4),\n",
    "            # nn.Linear(self.input_size*4, self.input_size*4, bias=False),\n",
    "            # nn.ReLU(),\n",
    "            # nn.BatchNorm1d(self.input_size*4), \n",
    "            # nn.Linear(self.input_size*4, self.input_size*8, bias=False),\n",
    "            # nn.ReLU(),\n",
    "            # nn.BatchNorm1d(self.input_size*8), #Again some says that it is not good to have Batch\n",
    "            # nn.Linear(self.input_size*8, self.input_size*8),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(self.input_size*8, self.output_size)\n",
    "            \n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ! Sanity Check, seems fine\n",
    "net = Network(258, 10)\n",
    "print(torch.randn((2,258)).dtype)\n",
    "net(torch.randn((2,258))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now the sad part as I had small amount of time and not very powerfull PC I need to cut off a lot of data :(\n",
    "batch_size = 128\n",
    "train_ds = ChemistryDataset(training_set[:100000], training_set_label[:100000])\n",
    "train_loader = DataLoader(train_ds, batch_size = batch_size, shuffle = True)\n",
    "val_ds = ChemistryDataset(validation_set[:10000], validation_set_label[:10000])\n",
    "validation_loader = DataLoader(val_ds, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quite standard setting, additionally some easy Learning rate shceduler\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "LEARNING_RATE = 0.001\n",
    "model = Network(285, 50).to(DEVICE).float()\n",
    "scaler = torch.cuda.amp.GradScaler()# This is used to scale gradients when they are to small to be represented in float16. Faster training!\n",
    "loss_fn = TripletLoss(margin=0.5, type=\"L2\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:07<00:00, 10.51it/s, loss=1.17] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data score: 1.1326602697372437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#At first lets check how data is separeted without any training in validation_loader. This may make no sense but if our model perform worse\n",
    "#after training on validation data than with random weights then we have a problem\n",
    "loop = tqdm(validation_loader, leave=True)\n",
    "val_losses = []\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (anchor, positive, negative, targets) in enumerate(loop):\n",
    "        anchor = anchor.to(DEVICE)\n",
    "        positive = positive.to(DEVICE)\n",
    "        negative = negative.to(DEVICE)\n",
    "        \n",
    "        loss = loss_fn(anchor, positive, negative)\n",
    "        \n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        val_losses.append(loss) \n",
    "\n",
    "print(f\"Raw data score: {sum(val_losses)/len(val_losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [02:44<00:00,  4.75it/s, loss=0.379]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:02<00:00, 33.97it/s, loss=0.324]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.4800557494163513, Validation Loss: 0.39670270681381226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [02:38<00:00,  4.94it/s, loss=0.335]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:02<00:00, 33.62it/s, loss=0.17] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss: 0.3729085922241211, Validation Loss: 0.38291510939598083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [02:32<00:00,  5.13it/s, loss=0.323]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:02<00:00, 33.49it/s, loss=0.342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss: 0.34989112615585327, Validation Loss: 0.39043399691581726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [02:31<00:00,  5.15it/s, loss=0.36] \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:02<00:00, 33.44it/s, loss=0.432]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss: 0.32847321033477783, Validation Loss: 0.3955220580101013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [02:35<00:00,  5.03it/s, loss=0.187]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:02<00:00, 33.49it/s, loss=0.471]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Training Loss: 0.3003995418548584, Validation Loss: 0.404936283826828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 66/782 [00:13<02:27,  4.85it/s, loss=0.236]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3136/991005394.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mloop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0manchor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpositive\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[1;31m#print(anchor.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0manchor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manchor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1184\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1185\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1186\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m                 \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3136/3689270802.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m#Now we need candidates for positive, But we must Exclude currently considered index!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mpositive_candidates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0manchor_label\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mpositive_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpositive_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mpositive\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpositive_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Training + Validation loops\n",
    "#Unfortunately model overfits a little bit, using more data for sure would be beneficial.\n",
    "\n",
    "for epoch in range(20):\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    model.train()    \n",
    "    for batch_idx, (anchor, positive, negative, targets) in enumerate(loop):\n",
    "        #Moving data to selected device\n",
    "        anchor = anchor.to(DEVICE)\n",
    "        positive = positive.to(DEVICE)\n",
    "        negative = negative.to(DEVICE)\n",
    "        \n",
    "        # This will allow doing some operations on float16 datatype. Morover this will make training faster\n",
    "        with torch.cuda.amp.autocast():\n",
    "            anchor_out = model(anchor)\n",
    "            positive_out = model(positive)\n",
    "            negative_out = model(negative)\n",
    "            loss = loss_fn(anchor_out, positive_out, negative_out)\n",
    "        \n",
    "        #We need to use scaler for gradient to be correct, since very small gradient may be set to 0 when cast to float 16\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        losses.append(loss)\n",
    "        \n",
    "    ## VALIDATION LOOP, almost the same stuff as for training, but we use torch_no_grad context manager and we do not calculate gradients\n",
    "    loop = tqdm(validation_loader, leave=True)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (anchor, positive, negative, targets) in enumerate(loop):\n",
    "            \n",
    "            anchor = anchor.to(DEVICE)\n",
    "            positive = positive.to(DEVICE)\n",
    "            negative = negative.to(DEVICE)\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                anchor_out = model(anchor)\n",
    "                positive_out = model(positive)\n",
    "                negative_out = model(negative)\n",
    "                loss = loss_fn(anchor_out, positive_out, negative_out)\n",
    "            \n",
    "            loop.set_postfix(loss=loss.item())\n",
    "            val_losses.append(loss) \n",
    "    \n",
    "    train_loss = sum(losses)/len(losses)\n",
    "    val_loss = sum(val_losses)/len(val_losses)\n",
    "    scheduler.step(val_loss)\n",
    "    torch.save(model.state_dict(), f\"TripletModel_{epoch}.pth\") #Save model from every epoch\n",
    "    print(f\"Epoch: {epoch+1}, Training Loss: {train_loss}, Validation Loss: {val_loss}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Network(285, 50).to(\"cuda\").float()\n",
    "model.load_state_dict(torch.load(\"TripletModel_2.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we need to project our training data to the lower dimensional space with our trained model\n",
    "model.eval()\n",
    "model.to(\"cpu\")\n",
    "training_predictors_trans = model(torch.tensor(training_set.values[:100000,],  dtype=torch.float32)).detach().numpy()\n",
    "for i in range(1,4):\n",
    "    next_part = model(torch.tensor(training_set.values[100000*i:100000*(i+1),],  dtype=torch.float32)).detach().numpy()\n",
    "    training_predictors_trans = np.concatenate((training_predictors_trans, next_part), axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We project validation data also\n",
    "validation_predictors_trans = model(torch.tensor(validation_set.values[:10000],  dtype=torch.float32)).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 50)\n",
      "(10000, 50)\n"
     ]
    }
   ],
   "source": [
    "#Sanity Check\n",
    "print(training_predictors_trans.shape)\n",
    "print(validation_predictors_trans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets try to learn xgboost on prepared data.\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV\n",
    "from sklearn import  metrics   #Additional scklearn functions\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, predictors, labels, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    #Here we try to choose best number of estimator as this is one of the most important parameters\n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(predictors, label=labels)\n",
    "        \n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='error', early_stopping_rounds=early_stopping_rounds, verbose_eval=False)\n",
    "        \n",
    "        print(cvresult.shape[0])\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    alg.fit(predictors, labels) \n",
    "    dtrain_predictions = alg.predict(predictors)\n",
    "    \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(f\"Accuracy : {metrics.accuracy_score(dtrain_predictions, labels)}\")  \n",
    "                    \n",
    "    # feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "    # feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    # plt.ylabel('Feature Importance Score')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314\n",
      "\n",
      "Model Report\n",
      "Accuracy : 0.82539\n"
     ]
    }
   ],
   "source": [
    "#I've chosen most popular parameters\n",
    "xgb1 = XGBClassifier(\n",
    " eval_metric=\"logloss\",\n",
    " use_label_encoder=False, \n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=3,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "modelfit(xgb1, training_predictors_trans[:100000], training_set_label[:100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After that I tried to perform some very simple Grid Search for next 2 most important parameters however this does not lead to better results\n",
    "param_test1 = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = XGBClassifier(\n",
    " eval_metric=\"logloss\",\n",
    " use_label_encoder=False, \n",
    " learning_rate =0.1,\n",
    " n_estimators=314, # I got this value from modelfit\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27), \n",
    " param_grid = param_test1, scoring='roc_auc',n_jobs=4, cv=5)\n",
    "gsearch1.fit(training_predictors_trans[:100000], training_set_label[:100000])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb1.save_model('xgboost3.model') # Lets save our model so that we can easily load it\n",
    "# xgb1 = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "# xgb1.load_model('xgboost3.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred =xgb1.predict(validation_predictors_trans) # getting predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7513\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_pred, validation_set_label[:10000]) # Getting accuracy score\n",
    "print(accuracy) # We overfit a little bit. Almost 8% difference. Probably fitting both models with full dataset would improve this by much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictors = test_data.drop([\"id\"], axis=1)# Mapping test predictors to numpy\n",
    "test_predictors = test_predictors.sub(training_mean, axis='columns').divide(training_std, axis='columns').values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cpu\")\n",
    "y_proba = xgb1.predict_proba(model(torch.tensor(test_predictors, dtype=torch.float32)).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = list(y_proba[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(test_data[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"submission.txt\", \"w\") as f:\n",
    "    f.write(\"id,target\\n\")\n",
    "    for i, score in zip(indices, probabilities):\n",
    "        f.write(f\"{i},{score}\\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "09061df0de86eb6b74b78d1f2f45a8c402c6eaf7467ea60364adcf0ec4d93469"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
